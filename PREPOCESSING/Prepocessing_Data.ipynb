{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3uxcw6CCuMLRpuk+E7gPV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXy5zkk1HZSV"
      },
      "outputs": [],
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install textblob\n",
        "!pip install sastrawi\n",
        "!pip install emoji\n",
        "!pip install PySastrawi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "import re\n",
        "import string\n",
        "from textblob import TextBlob\n",
        "import preprocessor as p\n",
        "from preprocessor.api import clean, tokenize, parse\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import emoji\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
      ],
      "metadata": {
        "id": "fmQE6KIPHjSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def load_data():\n",
        "  data = pd.read_csv('PILGUB.csv')\n",
        "  return data"
      ],
      "metadata": {
        "id": "jJkesQrFHlds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df = load_data()\n",
        "tweet_df"
      ],
      "metadata": {
        "id": "QMYZjANAH10n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning\n",
        "def remove_pattern(text, pattern_regex):\n",
        "  r = re.findall(pattern_regex, text)\n",
        "  for i in r:\n",
        "    text = re.sub(i, '', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "orM6_TmeH40o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df['clean_tweet'] = np.vectorize(remove_pattern)(tweet_df['Komentar'], \" *RT* | *@[\\w]*\")\n",
        "tweet_df.head(10)"
      ],
      "metadata": {
        "id": "U9pNvv0SH9BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove emoji\n",
        "def remove(text):\n",
        "  text =' '.join(re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "  return text\n",
        "tweet_df['remove_http'] = tweet_df['Komentar'].apply(lambda x: remove(x))\n",
        "tweet_df.head(10)"
      ],
      "metadata": {
        "id": "rDvoTPlpH_5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove\n",
        "def remov(text):\n",
        "  text = re.sub(r'r\\$\\w*', '', text)\n",
        "  text = re.sub(r'^RT[\\s]+', '', text)\n",
        "  text = re.sub(r'#', '', text)\n",
        "  text = re.sub(r'[0-9]+', '',text)\n",
        "\n",
        "  return text\n",
        "\n",
        "tweet_df['removing_hastag'] = tweet_df['remove_http'].apply(lambda x: remov(x))\n",
        "tweet_df.head(10)"
      ],
      "metadata": {
        "id": "vPUh6PoGIhYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove duplikat\n",
        "tweet_df.drop_duplicates(subset = \"removing_hastag\", keep = 'first', inplace = True)\n",
        "tweet_df.head()"
      ],
      "metadata": {
        "id": "XGuH_H-gIj2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi case folding\n",
        "def case_folding(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Menerapkan ke kolom DataFrame\n",
        "tweet_df['case_folding'] = tweet_df['removing_hastag'].apply(case_folding)\n",
        "\n",
        "# Cek hasil\n",
        "tweet_df[['removing_hastag', 'case_folding']].head()"
      ],
      "metadata": {
        "id": "0pReGnaiImNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan untuk stopword\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_indonesia = stopwords.words('indonesian')\n",
        "\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
        "\n",
        "# Ambil stopwords dari Sastrawi\n",
        "stop_factory = StopWordRemoverFactory().get_stop_words()\n",
        "\n",
        "# Tambahan stopwords gaul/custom\n",
        "more_stopwords = [\n",
        "    'yg', 'utk', 'cuman', 'deh', 'btw', 'tapi', 'gua', 'gue', 'lo', 'lu',\n",
        "    'kalo', 'trs', 'jd', 'nih', 'ntr', 'nya', 'lg', 'gk', 'ecusli', 'dpt',\n",
        "    'dr', 'kpn', 'kok', 'kyk', 'donk', 'yah', 'u', 'ya', 'ga', 'km', 'eh',\n",
        "    'sih', 'bang', 'br', 'rp', 'jt', 'kan', 'gpp', 'sm', 'usah',\n",
        "    'mas', 'sob', 'thx', 'ato', 'jg', 'gw', 'wkwkwk', 'mak', 'haha', 'iy', 'k',\n",
        "    'tp', 'haha', 'dg', 'dri', 'duh', 'ye', 'wkwk', 'syg',\n",
        "    'nerjemahin', 'gaes', 'guys', 'moga', 'kmrn', 'nemu', 'yukk',\n",
        "    'klas', 'iw', 'ew', 'lho', 'sbnry', 'org', 'gtu', 'bwt',\n",
        "    'krlga', 'clau', 'lbh', 'cpet', 'ku', 'wke', 'mba', 'sdh', 'oi',\n",
        "    'spt', 'dlm', 'bs', 'krn', 'jgn', 'sapa', 'sh', 'wakakaka',\n",
        "    'sihhh', 'hehe', 'ih', 'dgn', 'la', 'kl', 'ttg', 'mana', 'kmna', 'kmn',\n",
        "    'tdk', 'tuh', 'dah', 'kek', 'ko', 'pls', 'bbrp', 'pd', 'mah', 'dhhh',\n",
        "    'kpd', 'kzl', 'byar', 'si', 'sii', 'cm', 'sy', 'hahahaha', 'weh',\n",
        "    'dlu', 'tuhh', 'guweh', 'amp'\n",
        "]\n",
        "\n",
        "# Gabungkan semua stopwords\n",
        "combined_stopwords = stop_factory + more_stopwords\n",
        "\n",
        "# Buat dictionary & stopword remover\n",
        "dictionary = ArrayDictionary(combined_stopwords)\n",
        "stopword_remover = StopWordRemover(dictionary)\n",
        "\n",
        "# Fungsi untuk melakukan stopword removal\n",
        "def remove_stopwords(text):\n",
        "    return stopword_remover.remove(text)\n",
        "\n",
        "# Terapkan fungsi ke DataFrame dan simpan ke kolom baru\n",
        "tweet_df['stopword_removed'] = tweet_df['case_folding'].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "# Tampilkan hasil data terbaru\n",
        "print(tweet_df[['case_folding', 'stopword_removed']].head(10))"
      ],
      "metadata": {
        "id": "2nkIWWrwInGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df.head()"
      ],
      "metadata": {
        "id": "6z571Id7IqyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang dibutuhkan\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# Download the 'punkt' and 'punkt_tab' resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Fungsi untuk melakukan tokenizing (mengubah teks jadi list kata)\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Terapkan tokenizing dan simpan ke kolom baru\n",
        "tweet_df['tokenized'] = tweet_df['stopword_removed'].apply(lambda x: tokenize_text(x))\n",
        "\n",
        "# Konversi hasil token ke string dengan tanda kutip\n",
        "tweet_df['tokenized'] = tweet_df['tokenized'].apply(lambda tokens: \"[\" + \", \".join([repr(token) for token in tokens]) + \"]\")\n",
        "\n",
        "# Tampilkan hasil data terbaru\n",
        "print(tweet_df[['stopword_removed', 'tokenized']].head())"
      ],
      "metadata": {
        "id": "BcKgE7S-Iw2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df.head()"
      ],
      "metadata": {
        "id": "Rz94qbZcIywr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang dibutuhkan untuk stemming\n",
        "import pandas as pd\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Buat stemmer dari Sastrawi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Fungsi stemming\n",
        "def stem_text(text):\n",
        "    return stemmer.stem(text)\n",
        "\n",
        "# Simpan hasil stemming ke kolom baru\n",
        "tweet_df['stemmed'] = tweet_df['tokenized'].apply(lambda x: stem_text(x))\n",
        "\n",
        "# Tampilkan hasil data terbaru\n",
        "print(tweet_df[['tokenized', 'stemmed']])\n"
      ],
      "metadata": {
        "id": "NQnwx2ENJMIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df.head()"
      ],
      "metadata": {
        "id": "0Sq4H_UBJPOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simpan data bersih\n",
        "tweet_df.to_csv('stemming.csv', encoding ='utf8', index = False)"
      ],
      "metadata": {
        "id": "BnEPB9Q3JRY9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}